import openai
import os
import json
import re
from dotenv import load_dotenv
import pandas as pd

load_dotenv()
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def system_prompt():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content) 형태 기사.
출력: JSON (단, 아래 스키마 준수). 
규칙:

1) 한 기사당 items는 최대 2개(내용이 빈약하면 1개).
2) abridged_context는 원문을 요약·재구성하되 정답을 직접 지시하는 고유 수치, 날짜, 인명, 사건번호, 재판부 구성, 금액은 범주화/비식별화:
   - 금액: “수억 원대”, “수천만 원대 일부” 등.
   - 날짜: “해당 시기”, “이후” 등.
   - 인물: “[인물A]”, “[인물B]”.
   - 사건특정 식별자(판례 번호, 예: 2024도…): 제거.
3) question은 반드시 **추론형**으로 작성:
   - 단순 사실 확인 금지.
   - abridged_context 속 규범요소+사실요소를 결합해야 정답이 도출되도록 한다.
   - 법리적 근거(구성요건, 절차, 법리 판단, 법적 효과) 중심으로 설계.
4) choices는 5개. 정답은 1개만 존재해야 한다.
   - 오답은 “명백히 틀리지만 그럴듯”해야 함.
   - 오답 유형: (a) 구성요건 일부만 충족, (b) 절차/관할 오해, (c) 금액/시점 왜곡, (d) 인접 법령 혼동.
   - 모든 선택지는 **동일한 법적 층위**에서 경쟁해야 한다.
     (예: 모두 민법 내 법리, 모두 형사 구성요건 변형 등)
   - 전혀 무관한 분야(세금·상표권 등)는 금지.
5) leak_checks:
   - abridged_context에 정답 문자열이 그대로 포함되면 안 됨.
   - 원문 문장 그대로 복붙 금지(구문 50% 이상 변환).
   - 다중정답 금지.
6) 출력은 반드시 **순수 JSON만**. 코드블록(백틱)과 추가 텍스트 금지.
7) 정답·오답 모두 같은 법적 층위에서 경쟁하게 하되, 표면 단어(‘비보호좌회전’, ‘면책’ 등)만 바꾸면
   바로 걸러지는 보기 금지. 최소 2개 이상의 오답은 정답과 60% 이상 핵심 어휘가 겹치되,
   법리 pivot(요건 1개, 절차 단계, 책임 법적 성질 등)만 다르게 만든다.

8) 금지어 패턴(정답·오답 모두): "항상", "전부", "무조건", "면책된다"와 같은 절대화 표현.
   다만 판례상 예외가 없는 명시 규범인 경우에만 허용.

9) '문장 재진술형 정답' 금지: abridged_context의 문장을 거의 그대로 바꾼 보기(정답)는 금지.
   정답은 반드시 '법리(개념) → 사실 적용' 구조의 문장으로 작성한다.

10) 오답 유형 의무 비율:
    - 구성요건 누락/전도 1개 이상,
    - 절차·심급 오해 1개 이상,
    - 인접 법리 혼동(부당이득 vs 채무불이행 등) 1개 이상.
"""

def system_prompt_v2():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content) 형태 기사.
출력: JSON (단, 아래 스키마 준수). 
규칙:

1) 한 기사당 items는 최대 2개(내용이 빈약하면 1개).
2) abridged_context는 원문을 요약·재구성하되 정답을 직접 지시하는 고유 수치, 날짜, 인명, 사건번호, 재판부 구성, 금액은 범주화/비식별화:
   - 금액: “수억 원대”, “수천만 원대 일부” 등.
   - 날짜: “해당 시기”, “이후” 등.
   - 인물: “[인물A]”, “[인물B]”.
   - 사건특정 식별자(판례 번호, 예: 2024도…): 제거.
3) question은 반드시 **추론형**으로 작성:
   - 단순 사실 확인 금지.
   - abridged_context 속 규범요소+사실요소를 결합해야 정답이 도출되도록 한다.
   - 법리적 근거(구성요건, 절차, 법리 판단, 법적 효과) 중심으로 설계.
4) choices는 5개. 정답은 1개만 존재해야 한다.
   - 오답은 “명백히 틀리지만 그럴듯”해야 함.
   - 오답 유형: (a) 구성요건 일부만 충족, (b) 절차/관할 오해, (c) 금액/시점 왜곡, (d) 인접 법령 혼동.
   - 모든 선택지는 **동일한 법적 층위**에서 경쟁해야 한다.
     (예: 모두 민법 내 법리, 모두 형사 구성요건 변형 등)
   - 전혀 무관한 분야(세금·상표권 등)는 금지.
5) leak_checks:
   - abridged_context에 정답 문자열이 그대로 포함되면 안 됨.
   - 원문 문장 그대로 복붙 금지(구문 50% 이상 변환).
   - 다중정답 금지.
6) 출력은 반드시 유효 JSON만. 부가 텍스트 절대 금지.
7) 정답·오답 모두 같은 법적 층위에서 경쟁하게 하되, 표면 단어(‘비보호좌회전’, ‘면책’ 등)만 바꾸면
   바로 걸러지는 보기 금지. 최소 2개 이상의 오답은 정답과 60% 이상 핵심 어휘가 겹치되,
   법리 pivot(요건 1개, 절차 단계, 책임 법적 성질 등)만 다르게 만든다.

8) 금지어 패턴(정답·오답 모두): "항상", "전부", "무조건", "면책된다"와 같은 절대화 표현.
   다만 판례상 예외가 없는 명시 규범인 경우에만 허용.

9) '문장 재진술형 정답' 금지: abridged_context의 문장을 거의 그대로 바꾼 보기(정답)는 금지.
   정답은 반드시 '법리(개념) → 사실 적용' 구조의 문장으로 작성한다.

10) 오답 유형 의무 비율:
    - 구성요건 누락/전도 1개 이상,
    - 절차·심급 오해 1개 이상,
    - 인접 법리 혼동(부당이득 vs 채무불이행 등) 1개 이상.
11) 정답 피벗 비식별화: 정답을 결정짓는 핵심 속성(직책, 특정 행위, 조문번호, 심급 등)이 abridged_context에
    **그대로** 나타나면 안 된다. 해당 속성은 ‘범주표현’ 또는 ‘동의어+간접표현’으로만 서술한다.
    예) “국회 건설교통위원회 위원장” → “유관 상임위 소속 고위 공직자”.

12) 이중 전제 요구: question의 정답을 고르려면 **맥락의 최소 2개 사실 + 1개의 법리**를 결합해야 한다.
    (단일 사실 → 정답 금지)

13) 어휘 중첩 제한: 정답 선택지와 abridged_context의 **표면 어휘 중첩율**(토큰 기준)이 40%를 넘지 않게
    정답 문장을 재작성한다. (동의어·상위어 사용 권장)

14) 역유혹 단서 배치: abridged_context에 정답과 헷갈리는 ‘역유혹 신호’(예: 정치적 후원금, 일반 민원 등)를
    1개 포함하되, 그것이 합법·무관하다는 함의를 은근히 넣는다. (오답을 강화하기 위한 장치)

15) 오답 설계(동일 층위): 5지선다 중 최소 3개 오답은 **같은 법리 층위**에서 ‘요건 1개만 어긋난’ 형태로 작성한다.
    (대가성 부재, 직무관련성 단절, 사회상규상 허용되는 정치자금 등)

16) 절차/심급 오해 오답: 1개 오답은 사실심/법률심 권한 또는 보석·법정구속 요건을 일부만 맞게 섞어 만든다.

"""

def system_prompt_v3():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 

규칙:
1) 한 기사당 items는 1~2개.
2) abridged_context:
   - 원문 요약·재구성.
   - 인명, 직책, 사건번호, 금액 등은 범주/간접표현으로 비식별화.
   - 정답 피벗(핵심 속성)은 직접 노출 금지, 추론 단서만 남길 것.
3) question:
   - 단순 사실 확인 금지.
   - **최소 2개 사실 + 1개 법리** 결합해야 풀리도록 작성.
   - 정답은 "법리 개념 → 사실 적용" 구조.
4) choices:
   - 5지선다, 정답 1개.
   - 오답은 “그럴듯하지만 법리 pivot이 어긋난” 형태.
   - 최소: (a) 구성요건 누락/전도 1개, (b) 절차·심급 오해 1개, (c) 인접 법리 혼동 1개.
   - 모두 동일 법적 층위에서 경쟁.
5) 표현 제한:
   - “항상, 전부, 무조건, 면책된다” 같은 절대화 표현 금지(판례상 예외 외).
   - abridged_context 문장을 그대로 재진술한 정답 금지.
   - 정답·오답 간 표면 단어 중첩 40% 이하.
6) 출력은 반드시 **순수 JSON**만. 부가 텍스트 금지.
"""

# Best of (v1~v5)
def system_prompt_v4():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 최종 출력에는 부가 설명/코드블록 금지, **순수 JSON만**.

규칙(추론 강화):
1) 문항 수: 한 기사당 items 1~2개.
2) abridged_context(맥락 요약):
   - 원문을 재구성하되 인명/직책/사건번호/액수는 범주·간접표현으로 비식별화.
   - **정답 피벗(핵심 속성: 직책·구체 조문·심급 등)을 직접 노출하지 말고, 이를 유추할 수 있는 2개 이상의 간접 단서만 남긴다.**
   - 법적으로 무관하지만 그럴듯한 **미끼 사실 1개**를 포함하되, 맥락상 비핵심임을 은근히 시사할 것.
3) question(질문):
   - **단순 사실 확인 금지. 최소 2개 사실 + 1개 법리**(예: 구성요건/책임 범위/절차 법리) 결합 없이는 답을 못 내리게 작성.
   - 정답 문장은 **“법리 개념 → 사실 적용(두 사실 이상)”** 구조로 함(표현은 간결).
   - 동일 기사라도 문항 간 **법리 초점**을 다르게(예: 계약해석 vs 과실상계/인과관계 vs 절차/심급).
4) choices(보기):
   - 5지선다, 정답 1개. **모든 보기 동일 법적 층위**에서 경쟁(완전히 다른 분야 금지).
   - **근접오답(near-miss) 최소 3개**: 각 보기마다 정답 대비 법리 pivot을 1요소만 어긋나게 설계
     (예: 구성요건 일부 누락/전도, 책임 범위 오해, 인과관계 비약, 절차·심급 권한 오해).
   - **절차/심급 혼동 오답 1개**는 반드시 포함(사실심/법률심, 법정구속/보석, 항소 범위 등).
   - **절대어 금지:** “항상/전부/무조건/면책된다” 등(판례상 예외가 명시된 경우만 허용).
   - **표면 어휘 중첩 제한:** 정답과 abridged_context의 표면 어휘 중첩 ≤ 40%(동의어·상위어 사용).
   - **정답 재진술 금지:** abridged_context 문장 구조를 거의 그대로 따른 정답 문장 금지.
5) 난이도:
   - 기본 medium~hard. 단일 사실로 풀리면 안 됨. 근접오답과의 의미적 거리 좁히기.
6) 일관성·유일성:
   - **맥락의 단서들을 적용했을 때 정답만 참**이 되도록 보기를 조정(다중정답 금지).
7) 출력 형식:
   - 아래 스키마를 그대로 따르고, 최종 출력은 **순수 JSON만**.
"""

def system_prompt_v5():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 

규칙:
1) 한 기사당 items는 1~2개.

2) abridged_context:
   - 원문 요약·재구성.
   - 인명, 직책, 사건번호, 금액 등은 범주/간접표현으로 비식별화.
   - 정답 피벗(핵심 속성)은 직접 노출 금지, 추론 단서만 남길 것.

3) question:
   - 단순 사실 확인 금지.
   - **최소 2개 사실 + 1개 법리** 결합해야 풀리도록 작성.
   - 정답은 "법리 개념 → 사실 적용" 구조.
   - 유형은 두 가지 허용:
     (a) 단일 정답형 5지선다
     (b) "다음 중 옳은 것은?" 또는 "다음 중 틀린 것은?" 5지선다
   - "틀린 것" 유형은 오답이 명백히 판례·법리에 반하도록 작성.

4) choices:
   - 5지선다, 정답 1개.
   - 오답은 “그럴듯하지만 법리 pivot이 어긋난” 형태.
   - 최소: (a) 구성요건 누락/전도 1개, (b) 절차·심급 오해 1개, (c) 인접 법리 혼동 1개.
   - 모두 동일 법적 층위에서 경쟁.

5) 표현 제한:
   - “항상, 전부, 무조건, 면책된다” 같은 절대화 표현 금지(판례상 예외 외).
   - abridged_context 문장을 그대로 재진술한 정답 금지.
   - 정답·오답 간 표면 단어 중첩 40% 이하.

6) 출력은 반드시 **순수 JSON**만. 부가 텍스트 금지.
"""

# best of (~v7)
def system_prompt_v6():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 
규칙 :
0) 문항 유형: 'positive'(옳은 것 고르기)와 'negative'(옳지 않은 것 고르기) 유형을 적절히 혼합하여 생성합니다.

1) 문항 수: 한 기사당 items 1~2개.

2) abridged_context(맥락 요약):
   - 원문을 재구성하되 인명/직책/사건번호/액수는 범주·간접표현으로 비식별화합니다.
   - 보기들의 참/거짓을 판별할 핵심 근거(직책·구체 조문·심급 등)를 직접 노출하지 말고, 이를 유추할 수 있는 2개 이상의 간접 단서만 남깁니다.
   - 법적으로 무관하지만 그럴듯한 **미끼 사실 1개**를 포함하되, 맥락상 비핵심임을 은근히 시사해야 합니다.

3) question(질문):
   - **단순 사실 확인 금지**. 최소 2개 사실 + 1개 법리를 결합해야만 각 보기의 진위를 판별할 수 있도록 질문과 보기를 설계합니다.
   - 질문 텍스트는 `question_type`에 맞춰 'positive' 유형은 "~ 옳은 것은?", 'negative' 유형은 "~ 옳지 않은 것은?" 형식으로 명확히 합니다.
   - 동일 기사라도 문항 간 **법리 초점**을 다르게 설정합니다(예: 계약해석 vs 과실상계/인과관계 vs 절차/심급).

4) choices(보기):
   4-1) **공통 규칙**:
    - 5지선다, 정답 1개. **모든 보기는 동일한 법적 층위**에서 경쟁해야 합니다(완전히 다른 분야 금지).
    - 보기 중 최소 1개는 **절차/심급 혼동**을 유도하는 내용을 포함합니다(예: 사실심/법률심, 법정구속/보석, 항소 범위 등).
    - **절대어 금지**: “항상/전부/무조건/면책된다” 등의 사용을 피합니다(판례상 예외가 명시된 경우만 허용).
    - **표면 어휘 중첩 제한**: 보기와 abridged_context의 표면 어휘 중첩을 40% 이하로 유지합니다(동의어·상위어 사용).

   4-2) **유형별 규칙**:
    - **'positive' 유형 (옳은 것 고르기)**:
      - **정답 1개 + 근접오답(near-miss) 3개 이상**으로 구성.
      - 근접오답은 정답 대비 법리 pivot을 1요소만 미세하게 어긋나게 설계합니다(예: 구성요건 일부 누락, 인과관계 비약 등).
    - **'negative' 유형 (옳지 않은 것 고르기)**:
      - **정답(틀린 보기) 1개 + 옳은 보기 4개**로 구성.
      - **정답(틀린 보기)**은 근접오답 원리를 적용하여, 맥락의 단서와 법리를 결합했을 때 **미세하게 사실과 다르거나 법리 적용이 틀리도록** 설계합니다.
      - **옳은 보기 4개**는 맥락의 단서와 법리를 통해 **명확히 참으로 추론 가능**해야 합니다.

5) 난이도:
   - 기본 medium~hard. 단일 사실만으로 풀리지 않도록, 보기 간의 의미적 거리를 좁힙니다.

6) 일관성·유일성:
   - **맥락의 단서들을 모두 적용했을 때 정답만 유일하게 참('positive')이거나 거짓('negative')**이 되도록 보기를 조정하여 다중정답을 피합니다.

7) 출력 형식:
   - 위 스키마를 그대로 따르고, 최종 출력은 **순수 JSON만** 반환합니다.
"""

def system_prompt_v7():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 최종 출력에는 부가 설명/코드블록 금지, **순수 JSON만**.

규칙(추론 강화 + 문제유형 다양화):

1) 문항 수: 한 기사당 items 1~2개.

2) abridged_context(맥락 요약):
- 원문을 재구성하되 인명/직책/사건번호/액수는 범주·간접표현으로 비식별화.
- **정답 피벗(핵심 속성: 직책·구체 조문·심급 등)을 직접 노출하지 말고, 이를 유추할 수 있는 2개 이상의 간접 단서만 남긴다.**
- 법적으로 무관하지만 그럴듯한 **미끼 사실 1개**를 포함하되, 맥락상 비핵심임을 은근히 시사할 것.

3) question(질문):
- 유형: (a) "옳은 것은?" (b) "옳지 않은 것은?" 모두 허용. 기사별 최소 1개는 추론형.
- **단순 사실 확인 금지. 최소 2개 사실 + 1개 법리**(예: 구성요건/책임 범위/절차 법리) 결합 없이는 답을 못 내리게 작성.
- 정답 문장은 **“법리 개념 → 사실 적용(두 사실 이상)”** 구조로 함.
- 동일 기사라도 문항 간 **법리 초점**을 다르게(예: 계약해석 vs 과실상계/인과관계 vs 절차/심급).

4) choices(보기):
- 5지선다, 정답 1개. **모든 보기 동일 법적 층위**에서 경쟁(완전히 다른 분야 금지).
- **근접오답(near-miss) 최소 3개**: 정답 대비 법리 pivot을 1요소만 어긋나게 설계
  (예: 구성요건 일부 누락/전도, 책임 범위 오해, 인과관계 비약, 절차·심급 권한 오해).
- **절차/심급 혼동 오답 1개**는 반드시 포함.
- **절대어 금지:** “항상/전부/무조건/면책된다” 등(판례상 예외가 명시된 경우만 허용).
- **표면 어휘 중첩 제한:** 정답과 abridged_context의 표면 어휘 중첩 ≤ 40%(동의어·상위어 사용).
- **정답 재진술 금지:** abridged_context 문장을 그대로 베낀 정답 문장 금지.
- "옳지 않은 것" 문제에서는 1개만 확실히 틀리고, 나머지 4개 중 정답과 유사한 **근접오답**을 배치.

5) 난이도:
- 기본 medium~hard. 단일 사실로 풀리면 안 됨. 근접오답과의 의미적 거리 좁히기.

6) 일관성·유일성:
- **맥락 단서들을 종합했을 때 정답만 참**이 되도록 보기를 설계(다중정답 금지).

7) 출력 형식:
- 아래 스키마를 그대로 따르고, 최종 출력은 **순수 JSON만**.
"""

def system_prompt_v8():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 최종 출력에는 부가 설명/코드블록 금지, **순수 JSON만**.

규칙 (추론 강화 + 문제유형 다양화 + 미끼선지 고도화):

0) 문항 유형: 'positive'(옳은 것 고르기)와 'negative'(옳지 않은 것 고르기)를 적절히 혼합.

1) 문항 수: 한 기사당 items 1개.

2) abridged_context(맥락 요약):
   - 원문 재구성. 인명/직책/사건번호/액수는 범주·간접표현으로 비식별화.
   - 보기의 참/거짓을 가르는 **핵심 피벗(직책·구체 조문·심급·요건 등)**은 직접 노출하지 말고, 이를 유추하게 하는 **간접 단서 2개 이상**만 남긴다.
   - **법적으로 무관하지만 그럴듯한 ‘미끼 사실’ 1개**를 포함(예: 주변 맥락, 관행, 통계). 다만 문맥상 비핵심임을 은근히 시사.

3) question(질문):
   - **단순 사실 확인 금지. 최소 2개 사실 + 1개 법리**의 결합이 없으면 정답 판단이 불가하도록 작성.
   - 유형 문구 일치: positive는 “~ 옳은 것은?”, negative는 “~ 옳지 않은 것은?”.
   - 동일 기사 내 문항들은 **법리 초점**을 다르게(예: 구성요건 vs 책임 범위, 인과관계 vs 과실상계, 절차/심급 등).

4) choices(보기):
   4-1) 공통 규칙
     - 5지선다, 정답 1개. **모든 보기는 동일 법적 층위**에서 경쟁(다른 법 분야로 도피 금지).
     - 보기 중 **절차/심급 혼동** 유도형 **최소 1개** 포함(사실심/법률심, 항소 범위, 보석/법정구속 등).
     - **절대어 금지**: “항상/전부/무조건/면책된다”(명시적 예외 있는 판례·조문 제외).
     - **표면 어휘 중첩 제한**: 정답 문장과 abridged_context의 표면 단어 중첩 ≤ 40%(동의어·상위어 사용).
     - **정답 재진술 금지**: abridged_context 문장 구조를 거의 그대로 베낀 표현 금지.

   4-2) 미끼선지(강화) 설계 원칙 — 아래 중 **최소 3종**을 포함해 오답을 구성:
     - (Pivot-Shift) **요건 1개만 누락/전도**: 구성요건을 거의 맞추되 핵심 요소를 바꾸거나 빠뜨린다.
     - (Scope-Creep) **적용범위 과·소확장**: 해당 규범의 대상/시점/범위를 살짝 넓히거나 좁혀 틀리게.
     - (Procedure-Mixup) **절차·심급 오해**: 권한, 심사범위, 요건을 일부만 맞게 섞어 오류 유발.
     - (Adjacent-Law) **인접 법령 혼동**: 유사 규범(예: 민사/행정, 과세요건/부당이득)을 교묘히 섞되 조문/체계가 어긋나도록.
     - (Temporal/Quant) **시점·수량 왜곡**: 사실 시점 또는 정량 요소를 경미하게 틀리게.
     - (Authority-Misattribution) **권한 오인**: 기관/심급의 권한 또는 판단기준을 잘못 귀속.
     - (Benign-Fact Trap) **맥락의 미끼 사실**을 근거처럼 보이게 끼워 넣되, 법리와 무관하여 결국 오답이 되게.

     *가이드라인*: 오답 **최소 2개**는 정답과 핵심 어휘의 **60% 내외**를 공유(표면 유사)하되,
     **법리 pivot 1요소**만 다르게 만들어 **근접오답(near-miss)**로 설계한다.

   4-3) 유형별 규칙
     - **positive(옳은 것 고르기)**: 정답 1개 + **근접오답 3개 이상** 포함(위 미끼선지 원칙 활용).
     - **negative(옳지 않은 것 고르기)**: **틀린 보기 1개(정답)** + 옳은 보기 4개.
       - 틀린 보기는 **근접오답 원칙**을 적용해 “거의 맞지만 pivot이 틀린” 형태로 설계.
       - 나머지 4개는 맥락 단서와 법리로 **명확히 참**임을 추론 가능하게.

5) 난이도:
   - 기본 medium~hard. 단일 사실로 풀리지 않게 **보기 간 의미적 거리 좁히기**(정답/오답 간 의미 차이를 미세화).

6) 일관성·유일성(검증 기준):
   - 맥락 단서를 모두 적용하면 **positive는 정답만 참**, **negative는 정답만 거짓**이 되도록 조정(다중정답 금지).

7) 근거(reason):
   - 각 문항에 대해 **최대 100단어 이내**의 한국어 설명을 작성합니다.
   - 단순 결론 반복이 아니라, **맥락 요약 → 법리 → 정답 연결고리**를 짧게 드러내야 합니다.
   - 예: "대법원은 피보험자 변경 시 위험 증가 여부를 고려하지 않는 약관은 무효라고 판시했으므로, 단순히 보험사 승인 없이는 계약이 실효된다는 조항은 인정되지 않는다."

8) 출력 형식:
   - 제공된 JSON 스키마를 그대로 따르고, 최종 출력은 **순수 JSON만** 반환합니다.
"""

def system_prompt_v9():
    return """
당신은 한국어 법률 교육용 문제 생성기입니다.
입력: (url, title, content).
출력: JSON (아래 스키마 준수). 최종 출력에는 부가 설명/코드블록 금지, **순수 JSON만**.

규칙 :
0) 문항 유형: 'positive'(옳은 것 고르기)와 'negative'(옳지 않은 것 고르기) 유형을 적절히 혼합하여 생성합니다.

1) 문항 수: 한 기사당 items 1개.

2) abridged_context(맥락 요약):
   - 원문을 재구성하되 인명/직책/사건번호/액수는 범주·간접표현으로 비식별화합니다.
   - 보기들의 참/거짓을 판별할 **핵심 근거(직책·구체 조문·심급·핵심 요건 등)**는 직접 노출하지 말고, 이를 유추할 수 있는 **간접 단서 2개 이상**만 남깁니다.
   - **비핵심 ‘미끼 사실(benign decoy facts)’을 1~2개 포함**합니다:
     * 법적 판단에 **결정적이지 않지만 그럴듯한 주변 정보**(관행·일반 절차·장소/시간 배경·행정 편의·통계적 인상 등).
     * **핵심 피벗과 직접적 인과관계가 없어야** 하며, 정답의 참/거짓을 **단독으로 좌우하지 못해야** 합니다.
     * **신호 처리**: “통상적으로”, “관례상”, “일반적으로”, “다만 별도 사정이 없는 한” 등의 표현으로 **비핵심임을 은근히 시사**합니다.
     * **금지**: (a) 새로운 법적 요건/예외 창설, (b) 판결 취지를 바꾸는 단서, (c) 특정 수치/날짜/고유명사로 정답을 직접 특정, (d) 핵심 피벗(조문·심급·직책)의 노골적 노출.

3) question(질문):
   - **단순 사실 확인 금지**. 최소 **2개 사실 + 1개 법리**를 결합해야만 각 보기의 진위를 판별할 수 있도록 질문과 보기를 설계합니다.
   - `question_type`에 맞춰: 'positive'는 “~ 옳은 것은?”, 'negative'는 “~ 옳지 않은 것은?” 형식으로 명확히 작성합니다.
   - 동일 기사라도 문항 간 **법리 초점**을 다르게 설정합니다(예: 계약해석 vs 과실상계/인과관계 vs 절차/심급).

4) choices(보기):
   4-1) **공통 규칙**:
    - 5지선다, 정답 1개. **모든 보기는 동일한 법적 층위**에서 경쟁해야 합니다(완전히 다른 분야 금지).
    - 보기 중 **절차/심급 혼동** 유도형 **최소 1개** 포함(사실심/법률심, 항소 범위, 보석/법정구속 등).
    - **절대어 금지**: “항상/전부/무조건/면책된다”(판례상 예외 명시 시만 허용).
    - **표면 어휘 중첩 제한**: 보기와 abridged_context의 표면 어휘 중첩을 **40% 이하**로 유지(동의어·상위어 사용).
    - **정답 재진술 금지**: abridged_context 문장 구조를 거의 그대로 따른 정답 문장 금지.

   4-2) **유형별 규칙**:
    - **'positive' 유형 (옳은 것 고르기)**:
      - **정답 1개 + 근접오답(near-miss) 3개 이상**으로 구성.
      - 근접오답은 정답 대비 **법리 pivot 1요소만 미세하게** 어긋나게 설계(요건 일부 누락·전도, 인과관계 비약, 적용범위 과·소확장 등).
      - **미끼 사실을 근거로 착시를 유도**하되, 최종 판단은 법리+핵심 단서 결합이 필요하도록 구성.
    - **'negative' 유형 (옳지 않은 것 고르기)**:
      - **정답(틀린 보기) 1개 + 옳은 보기 4개**.
      - 정답(틀린 보기)은 **근접오답 원칙**으로 “거의 맞지만 pivot이 틀린” 형태.
      - 옳은 보기 4개는 **맥락 단서+법리**로 명확히 참으로 귀결.

5) 난이도:
   - 기본 medium~hard. **단일 사실**만으로 풀리지 않게 보기 간 의미적 거리 좁히기(정답/오답 간 의미 차이를 미세화).

6) 일관성·유일성:
   - **맥락 단서를 모두 적용했을 때** 'positive'는 **정답만 참**, 'negative'는 **정답만 거짓**이 되도록 조정(다중정답 금지).

7) 근거(reason):
   - 각 문항에 대해 **최대 100단어 이내**의 한국어 설명을 작성합니다.
   - 단순 결론 반복이 아니라, **맥락 요약 → 법리 → 정답 연결고리**를 짧게 드러내야 합니다.
   - 예: "대법원은 피보험자 변경 시 위험 증가 여부를 고려하지 않는 약관은 무효라고 판시했으므로, 단순히 보험사 승인 없이는 계약이 실효된다는 조항은 인정되지 않는다."


8) 출력 형식:
   - 제공된 JSON 스키마를 그대로 따르고, 최종 출력은 **순수 JSON만** 반환합니다.
"""

def system_prompt_v10():
    return """
당신은 법학 교육용 객관식 문항(MCQA)을 출제하는 전문가이며, 
법전원(법학전문대학원) 수준의 엄격한 기준에 맞춰 기사 기반 문항을 생성해야 한다. 
생성물은 학습자의 법리 추론 능력(맥락 해석 + 법리 적용)을 평가해야 하며, 
정답은 반드시 하나만 존재해야 한다. 
오답(미끼선지)은 그럴듯하지만 잘못된 근접오답(near-miss)으로 구성한다.

입력: (url, title, content)
출력: JSON (아래 스키마 준수). 최종 출력에는 부가 설명/코드블록 금지, **순수 JSON만**.

------------------------------------------------------
[STEP 1] 핵심 추출 (Issue & Pivot)
1. 기사 전체를 읽고 핵심 법적 쟁점(issue)을 최대 3개까지 추출한다.
2. 각 쟁점에 대한 핵심 법리(pivot rule)를 한 문장으로 요약한다.
3. 출제할 1개 쟁점을 선택하고 이유를 간단히 적는다.

출력 예:
{
  "issues": [{"issue":"...", "pivot":"..."}],
  "selected_issue_index": 0
}

------------------------------------------------------
[STEP 2] abridged_context 생성 (v9 스타일)
- 인명·지명·사건번호 등 식별정보는 모두 비식별화한다.
- 정답의 근거가 되는 핵심 정보를 직접 노출하지 않고, 
  추론 가능한 간접 단서를 최소 2개 이상 포함한다.
- 길이는 80~180단어 수준으로 유지한다.
- 판결문 문구를 그대로 복사하지 말고 재서술(paraphrase)한다.
- 핵심 pivot의 직설적 표현을 피한다.

출력:
"abridged_context": "...",
"context_sent_map": [문장번호...]

------------------------------------------------------
[STEP 3] 질문(question) 설계
- 유형: 기본은 "analysis", 필요시 "application"은 새로운 가상사례를 제시한다.
- 질문은 단일 사실에 기반하지 않도록, 
  최소 2개의 사실 요소 + 1개의 법리 요소가 결합되어야 한다.
- 형식: “어떤 판단이 타당한가?”, “다음 중 옳은 것은?” 등 명확하고 간결하게 작성.

출력:
"question": "..."

------------------------------------------------------
[STEP 4] 선택지(choices) 생성

- 5지선다형(정답 1개, 오답 4개)으로 구성한다.
- 모든 선택지는 동일한 법적 층위(예: 구성요건 판단, 위법성 판단, 책임 판단 등)에서 경쟁해야 한다.
- 각 보기의 문장 길이와 복잡도는 유사하게 유지한다.
- 정답은 오직 하나이며, 논리적으로 완결된 판단이어야 한다.
- 정답 선택지는 랜덤하게 선택한다.
- 오답은 정답과의 의미적 거리가 가깝지만 논리적으로 틀린 근거를 가져야 한다.
  (즉, "틀린 이유는 작지만 분명해야 한다.")
- 모든 오답은 기사 본문 내 근거 문장(ref_sent_ids)을 최소 1개 이상 가져야 한다.
- 기사 외의 사실, 법조문, 판례를 새로 만들어내지 말아야 한다. (환각 금지)

오답 설계 시 다음 원칙을 모두 적용한다.
1. **Pivot-Shift** — 구성요건의 핵심 요소 1개를 누락하거나 전도한다.  
2. **Scope-Creep** — 법리의 적용 범위를 과도하게 확장하거나 축소한다.  
3. **Procedure-Mixup** — 절차 요건과 실체 요건을 혼동한다.  
4. **Benign-Fact Trap** — 비핵심 사실을 법적 근거로 오인하게 유도한다.  
5. **Near-Miss Reinforcement (신규)** — 정답의 논리 흐름을 70%까지 유지하되 pivot을 잘못 적용하도록 설계한다.  
6. **Plausibility Balance (신규)** — 정답 대비 오답의 문체, 논리 구조, 문장 길이가 유사해야 한다.  

어휘 중첩률 제한:
- 정답 문장과 abridged_context 간 핵심 명사·동사 기준 중첩률은 40% 이하.
- 동일한 핵심 법리 표현(pivot word)은 1회 이하로 반복.

------------------------------------------------------
[STEP 5] 정답·오답 검증 루틴
- 정답 유일성 검증: 5개 보기 중 오직 하나만 논리적으로 참인지 판단한다.
- 각 선택지에 대해 참/거짓/불확실을 평가하고, 참이 1개인지 확인한다.
- 정답을 지지하는 문장번호를 evidence_sent_ids로 표기한다.
- 각 오답에 대해 왜 틀렸는지(한 문장 요약)와 참조 문장번호(ref_sent_ids)를 제시한다.
- abridged_context와 정답 간 어휘 중첩률이 40%를 초과하면 보기를 재작성한다.
- 중복·모순 보기가 있으면 수정한다.
- 최대 3회까지 자동 재생성 허용.

------------------------------------------------------
[STEP 6] 해설(explanation)
- 구조: 근거 요약(1~2문장) → 법리 제시(1문장) → 사실-법리 연결(2~3문장)
- 길이: 120단어 이하
- 마지막에 참고 문장번호(evidence_sent_ids)를 명시한다.

------------------------------------------------------
[STEP 8] 난이도 평가 (LLM Judge 연계 및 자동 보정)
- 각 문항에 대해 난이도를 1~5 점수로 평가하라.
- 평가 기준은 다음과 같다.
  1점: 단순 암기형. pivot 문장만 기억하면 정답 도출 가능.
  2점: 기본 법리를 알고 단일 단계 추론으로 해결 가능.
  3점: 2~3단계 추론 필요. 사실+법리의 결합이 요구됨.
  4점: 예외 법리·판례 경계 구분 등 복합적 추론 필요.
  5점: 사실·법리의 충돌 또는 반례 사고 요구. 고난도 응용형.
- 각 문항에 대해 "difficulty_score"(정수)와 "difficulty_reason"(1~2문장)을 함께 출력한다.
- 난이도 스코어는 LLM Judge 후처리 단계에서 다음 기준으로 필터링된다:
  - 1점 또는 5점으로 평가된 문항은 데이터셋에서 제외(flag_for_removal=true).
  - 2~4점만 유지.
- Judge는 추가적으로 다음 항목을 재검증한다:
  (1) 정답 유일성
  (2) 논리적 근거의 명확성 (evidence_sent_ids 존재 여부)
  (3) 선택지 간 의미 중복 여부
  (4) 환각(hallucination) 발생 여부 (기사 외 정보 등장 금지)

------------------------------------------------------
[STEP 9] 최종 품질 검증 및 구조화 출력 (Quality Assurance)
- 모든 생성물이 아래 조건을 만족해야 한다.
  1. 정답 유일성(unique_correct == true)
  2. 증거 문장(evidence_sent_ids) 존재
  3. 어휘 중첩률(lexical_overlap_estimate_pct ≤ 40)
  4. 해설 길이(explanation_length ≤ 120단어)
  5. 오답 근거 명시(distractor_rationales 존재)
  6. 난이도 점수 2~4 범위
- 위 조건 중 하나라도 불충족 시 "validation_failure_reasons"에 원인 코드를 추가하고,
  자동 수정 절차(auto_repair loop)를 최대 5회까지 실행한다.


출력: JSON
"""

json_schema = {
    "meta": {
        "url": "<string>",
        "title": "<string>",
        "num_question": "1"
    },
    "items": [
        {
            "abridged_context": "<정답 단서 제거·축약 본문 (최대 180단어, 한국어)>",
            "question": "<5지선다 단일정답형 질문 (한국어)>",
            "choices": {
                "A": "<텍스트>",
                "B": "<텍스트>",
                "C": "<텍스트>",
                "D": "<텍스트>",
                "E": "<텍스트>"
            },
            "correct": "A|B|C|D|E",
            "reason" : "<정답 근거 (최대 100단어, 한국어)>",
        }
    ]
}

def user_prompt(url, title, content):
    return f"""
다음 기사를 바탕으로 스키마에 맞춰 문제를 생성해줘.
url : {url}
title: {title}
content: {content}

아래 스키마를 따르고, **순수 JSON만** 출력해줘(코드블록 금지).
{json.dumps(json_schema, ensure_ascii=False)}
"""

def strip_code_fences(s: str) -> str:
    s = re.sub(r"^\s*```(?:json)?\s*", "", s.strip())
    s = re.sub(r"\s*```\s*$", "", s)
    return s.strip()

def call_gpt_with_prompts(system_prompt_text, user_prompt_text, model="gpt-4.1"):
    contexts = [
        {"role": "system", "content": system_prompt_text},
        {"role": "user", "content": user_prompt_text}
    ]
    response = client.chat.completions.create(
        model=model,
        messages=contexts,
        response_format={"type": "json_object"}  # 지원 안 되면 제거
    )
    return response.choices[0].message.content

def parse_json_or_raise(raw: str):
    cleaned = strip_code_fences(raw)
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        # 모델이 앞뒤에 설명을 붙였을 가능성까지 대비: 중괄호 블록만 추출
        m = re.search(r"\{.*\}\s*$", cleaned, re.DOTALL)
        if m:
            return json.loads(m.group(0))
        raise e

def validate_json_schema(obj: dict):
    # 필수 키 검사
    for k in ["meta", "items"]:
        assert k in obj, f"missing key: {k}"
    for k in ["url", "title", "num_question"]:
        assert k in obj["meta"], f"missing meta.{k}"
    assert isinstance(obj["items"], list) and len(obj["items"]) >= 1, "items must be non-empty list"

    # num_question 일치 검사 (문자열 "1"로 온다는 전제)
    try:
        n = int(obj["meta"]["num_question"])
        assert n == 1, "num_question must be '1'"
        assert len(obj["items"]) == n, f"items length {len(obj['items'])} != num_question {n}"
    except Exception:
        raise AssertionError("num_question must be convertible to int 1")

    # 각 문항 필드 검사
    for i, it in enumerate(obj["items"]):
        for k in ["abridged_context", "question", "choices", "correct"]:
            assert k in it, f"items[{i}].{k} missing"
        assert isinstance(it["choices"], dict) and set(it["choices"].keys()) == {"A","B","C","D","E"}, "choices must have A~E"
        assert it["correct"] in {"A","B","C","D","E"}, "correct must be one of A~E"

if __name__ == "__main__":
    df = pd.read_csv("./dataset/raw/lawtimes/lawtimes_all.csv")
    sample = df.sample(1).iloc[0]
    url, title, content = sample['url'], sample['title'], sample['content']

    # v8
    raw = call_gpt_with_prompts(system_prompt_v8(), user_prompt(url, title, content))
    try:
        obj = parse_json_or_raise(raw)
        validate_json_schema(obj)
    except Exception as e:
        print("파싱/검증 에러:", e)
        raise

    with open("output.json", "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

    # v9
    raw = call_gpt_with_prompts(system_prompt_v9(), user_prompt(url, title, content))
    try:
        obj = parse_json_or_raise(raw)
        validate_json_schema(obj)
    except Exception as e:
        print("파싱/검증 에러:", e)
        raise

    with open("output.json", "a", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    print("output.json 저장 완료")
